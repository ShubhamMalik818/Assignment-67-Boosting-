{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63f754-b2a8-4811-ad5c-efdaf41bd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "ANS- Gradient boosting regression is a machine learning algorithm that combines multiple weak learners to create a strong learner. The weak learners \n",
    "     are typically decision trees, and they are trained sequentially. Each weak learner is trained to correct the errors of the previous weak \n",
    "     learners. This helps to reduce bias and variance in the final model.\n",
    "\n",
    "Gradient boosting regression is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning \n",
    "models. It is a versatile algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "Here are some of the advantages of gradient boosting regression:\n",
    "\n",
    "1. Accuracy: Gradient boosting regression can often achieve high accuracy, especially for difficult problems.\n",
    "2. Robustness: Gradient boosting regression is a robust algorithm that can be used to handle noise and outliers in the data.\n",
    "3. Interpretability: Gradient boosting regression models can be more interpretable than some other machine learning models. This can be helpful for \n",
    "                     understanding how the model works and for debugging the model.\n",
    "4. Scalability: Gradient boosting regression models can be scaled to large data sets.\n",
    "\n",
    "\n",
    "Here are some of the disadvantages of gradient boosting regression:\n",
    "\n",
    "1. Complexity: Gradient boosting regression models can be more complex than some other machine learning models. This can make them more difficult to \n",
    "               understand and to interpret.\n",
    "2. Computational complexity: Gradient boosting regression models can be computationally expensive to train, especially for large data sets.\n",
    "3. Overfitting: Gradient boosting regression models can be prone to overfitting if the hyperparameters are not tuned carefully.\n",
    "\n",
    "\n",
    "Overall, gradient boosting regression is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine \n",
    "learning models. It is a versatile algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "Here are some of the key concepts of gradient boosting regression:\n",
    "\n",
    "1. Weak learners: The weak learners are simple models that are easy to train. This makes them less prone to overfitting the data.\n",
    "2. Gradient descent: Gradient descent is an iterative optimization algorithm that is used to train the weak learners.\n",
    "3. Loss function: The loss function measures the accuracy of the model. The most common loss function for gradient boosting regression is the mean \n",
    "                  squared error function.\n",
    "4. Regularization: Regularization is a technique used to prevent the model from overfitting the data. There are many different regularization \n",
    "                   techniques, but some of the most common include L1 regularization and L2 regularization.\n",
    "\n",
    "Gradient boosting regression is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning \n",
    "models. It is a versatile algorithm that can be used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0adcb-63ea-4df2-94e2-661ebb93ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the \n",
    "    model on a small dataset. Evaluate the model performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5f07b-ee97-4edc-a0b6-b44aaa3964d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_boosting(X, y, learning_rate=0.1, n_estimators=10):\n",
    "  \"\"\"\n",
    "  Gradient boosting algorithm implemented from scratch.\n",
    "\n",
    "  Args:\n",
    "    X: The training data.\n",
    "    y: The target values.\n",
    "    learning_rate: The learning rate.\n",
    "    n_estimators: The number of estimators.\n",
    "\n",
    "  Returns:\n",
    "    The trained model.\n",
    "  \"\"\"\n",
    "\n",
    "  models = []\n",
    "    for i in range(n_estimators):\n",
    "        error = y - models[-1].predict(X)\n",
    "        model = DecisionTreeRegressor(max_depth=1)\n",
    "        model.fit(X, error)\n",
    "        models.append(model)\n",
    "\n",
    "    predictions = np.zeros_like(y)\n",
    "    for model in models:\n",
    "        predictions += model.predict(X)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    y = 2 * X + np.random.randn(100)\n",
    "\n",
    "    predictions = gradient_boosting(X, y)\n",
    "\n",
    "    print(\"Mean squared error:\", np.mean((predictions - y)**2))\n",
    "    print(\"R-squared:\", np.corrcoef(predictions, y)[0, 1]**2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc89252-9851-4378-b6b7-ca9deb114d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code implements a simple gradient boosting algorithm that uses decision trees as the weak learners. The algorithm is trained on a simple \n",
    "regression problem, and the performance of the model is evaluated using mean squared error and R-squared.\n",
    "\n",
    "The output of the code will show the mean squared error and R-squared of the model.\n",
    "\n",
    "This is just a simple example of how to implement a gradient boosting algorithm from scratch. There are many other ways to implement this algorithm, \n",
    "and there are also many different ways to tune the hyperparameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525e982-cac7-477f-b39f-597a3b95e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use \n",
    "    grid search or random search to find the best hyperparameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26af670-c384-4c68-9627-6684ae70aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def gradient_boosting(X, y, learning_rate, n_estimators, max_depth):\n",
    "  \"\"\"\n",
    "  Gradient boosting algorithm implemented from scratch.\n",
    "\n",
    "  Args:\n",
    "    X: The training data.\n",
    "    y: The target values.\n",
    "    learning_rate: The learning rate.\n",
    "    n_estimators: The number of estimators.\n",
    "    max_depth: The maximum depth of the trees.\n",
    "\n",
    "  Returns:\n",
    "    The trained model.\n",
    "  \"\"\"\n",
    "\n",
    "  models = []\n",
    "    for i in range(n_estimators):\n",
    "        error = y - models[-1].predict(X)\n",
    "        model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        model.fit(X, error)\n",
    "        models.append(model)\n",
    "\n",
    "    predictions = np.zeros_like(y)\n",
    "    for model in models:\n",
    "        predictions += model.predict(X)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    y = 2 * X + np.random.randn(100)\n",
    "\n",
    "    param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [1, 3, 5],\n",
    "  }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "       estimator=gradient_boosting,\n",
    "       param_grid=param_grid,\n",
    "       cv=5,\n",
    "       scoring=\"neg_mean_squared_error\",\n",
    "  )\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfd660-5201-45ad-aeb2-3e913db52fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code uses grid search to find the best hyperparameters for a gradient boosting algorithm. The hyperparameters that are tuned are the learning \n",
    "rate, the number of estimators, and the maximum depth of the trees. The code first defines a parameter grid that specifies the values of the \n",
    "hyperparameters that will be tuned. Then, the code creates a grid search object and fits it to the data. The grid search object will then find the \n",
    "best hyperparameters for the model. The best hyperparameters and the best score are then printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3a8ea-acd1-4639-9f48-cfcb1f6f0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "ANS- In gradient boosting, a weak learner is a simple model that is easy to train. This makes them less prone to overfitting the data. \n",
    "     The weak learners are typically decision trees, but they can also be other types of models.\n",
    "\n",
    "The weak learners are trained sequentially. Each weak learner is trained to correct the errors of the previous weak learners. This helps to reduce \n",
    "bias and variance in the final model.\n",
    "\n",
    "The final model is a combination of the weak learners. The weights of the weak learners are determined by a process called gradient descent.\n",
    "\n",
    "Here are some of the advantages of using weak learners in gradient boosting:\n",
    "\n",
    "1. Accuracy: Gradient boosting can often achieve high accuracy, especially for difficult problems.\n",
    "2. Robustness: Gradient boosting is a robust algorithm that can be used to handle noise and outliers in the data.\n",
    "3. Interpretability: Weak learners can be more interpretable than some other machine learning models. This can be helpful for understanding how the \n",
    "                     model works and for debugging the model.\n",
    "4. Scalability: Gradient boosting models can be scaled to large data sets.\n",
    "\n",
    "\n",
    "Here are some of the disadvantages of using weak learners in gradient boosting:\n",
    "\n",
    "1. Complexity: Gradient boosting models can be more complex than some other machine learning models. This can make them more difficult to understand \n",
    "               and to interpret.\n",
    "2. Computational complexity: Gradient boosting models can be computationally expensive to train, especially for large data sets.\n",
    "3. Overfitting: Gradient boosting models can be prone to overfitting if the hyperparameters are not tuned carefully.\n",
    "\n",
    "Overall, weak learners are a powerful tool that can be used to improve the accuracy of gradient boosting models. They are simple to train, robust to \n",
    "noise, and can be scaled to large data sets. \n",
    "However, they can also be complex and prone to overfitting, so it is important to tune the hyperparameters carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c236e-a075-4810-b611-42dc6819a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "ANS- Gradient boosting is an ensemble learning algorithm that combines multiple weak learners to create a strong learner. The weak learners are \n",
    "     typically decision trees, and they are trained sequentially. Each weak learner is trained to correct the errors of the previous weak learners. \n",
    "    This helps to reduce bias and variance in the final model.\n",
    "\n",
    "The intuition behind gradient boosting is that the errors of the previous weak learners can be used to train a new weak learner that will make \n",
    "fewer errors. This process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "Here is an example of how gradient boosting works:\n",
    "\n",
    "Suppose we have a data set with 100 samples. The first weak learner is trained to predict the target values. The weak learner will make some mistakes, \n",
    "but it will also make some correct predictions.\n",
    "\n",
    "The errors of the weak learner are then used to train a new weak learner. The new weak learner is trained to correct the errors of the first weak \n",
    "learner. This means that the new weak learner will focus more on the samples that the first weak learner made mistakes on.\n",
    "\n",
    "This process is repeated until the desired accuracy is achieved. The final model is a combination of the weak learners. The weights of the weak \n",
    "learners are determined by a process called gradient descent.\n",
    "\n",
    "Gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. It is a \n",
    "versatile algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "\n",
    "Here are some of the advantages of gradient boosting:\n",
    "\n",
    "1. Accuracy: Gradient boosting can often achieve high accuracy, especially for difficult problems.\n",
    "2. Robustness: Gradient boosting is a robust algorithm that can be used to handle noise and outliers in the data.\n",
    "3. Interpretability: Gradient boosting models can be more interpretable than some other machine learning models. This can be helpful for understanding \n",
    "                     how the model works and for debugging the model.\n",
    "4. Scalability: Gradient boosting models can be scaled to large data sets.\n",
    "\n",
    "Here are some of the disadvantages of gradient boosting:\n",
    "\n",
    "1. Complexity: Gradient boosting models can be more complex than some other machine learning models. This can make them more difficult to understand \n",
    "               and to interpret.\n",
    "2. Computational complexity: Gradient boosting models can be computationally expensive to train, especially for large data sets.\n",
    "3. Overfitting: Gradient boosting models can be prone to overfitting if the hyperparameters are not tuned carefully.\n",
    "\n",
    "Overall, gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. \n",
    "However, it is important to be aware of the potential disadvantages of this algorithm before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc22b1-4079-40f6-bbda-fc41da9a733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "ANS- Gradient boosting builds an ensemble of weak learners by sequentially adding new learners to the ensemble. Each new learner is trained to \n",
    "     correct the errors of the previous learners. This process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "The weak learners in gradient boosting are typically decision trees. Decision trees are simple models that are easy to train and interpret. \n",
    "However, they can be prone to overfitting. Gradient boosting addresses this issue by training the weak learners sequentially.\n",
    "\n",
    "The first weak learner is trained to predict the target values. The errors of the first weak learner are then used to train a new weak learner. \n",
    "The new weak learner is trained to correct the errors of the first weak learner. This means that the new weak learner will focus more on the samples \n",
    "that the first weak learner made mistakes on.\n",
    "\n",
    "This process is repeated until the desired accuracy is achieved. The final model is a combination of the weak learners. The weights of the weak \n",
    "learners are determined by a process called gradient descent.\n",
    "\n",
    "\n",
    "Here are some of the steps involved in building an ensemble of weak learners using gradient boosting:\n",
    "\n",
    "1. Initialize the ensemble with a weak learner.\n",
    "2. Calculate the errors of the ensemble on the training data.\n",
    "3. Train a new weak learner to correct the errors of the ensemble.\n",
    "4. Add the new weak learner to the ensemble.\n",
    "5. Repeat steps 2-4 until the desired accuracy is achieved.\n",
    "\n",
    "Gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. \n",
    "It is a versatile algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "\n",
    "Here are some of the advantages of gradient boosting:\n",
    "\n",
    "1. Accuracy: Gradient boosting can often achieve high accuracy, especially for difficult problems.\n",
    "2. Robustness: Gradient boosting is a robust algorithm that can be used to handle noise and outliers in the data.\n",
    "3. Interpretability: Weak learners can be more interpretable than some other machine learning models. This can be helpful for understanding how \n",
    "                     the model works and for debugging the model.\n",
    "4. Scalability: Gradient boosting models can be scaled to large data sets.\n",
    "\n",
    "\n",
    "Here are some of the disadvantages of gradient boosting:\n",
    "\n",
    "1. Complexity: Gradient boosting models can be more complex than some other machine learning models. This can make them more difficult to understand \n",
    "               and to interpret.\n",
    "2. Computational complexity: Gradient boosting models can be computationally expensive to train, especially for large data sets.\n",
    "3. Overfitting: Gradient boosting models can be prone to overfitting if the hyperparameters are not tuned carefully.\n",
    "\n",
    "Overall, gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. \n",
    "However, it is important to be aware of the potential disadvantages of this algorithm before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d7dea-628e-4b3d-ad8a-563c62d66e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "ANS- Here are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm:\n",
    "\n",
    "1. Define the loss function. The loss function is a measure of the error between the predictions of the model and the actual values. The most \n",
    "   common loss function for gradient boosting is the mean squared error.\n",
    "2. Initialize the model. The model is initialized with a weak learner. The weak learner can be any type of model, but decision trees are typically \n",
    "   used.\n",
    "3. Calculate the gradient of the loss function. The gradient of the loss function is a vector that points in the direction of the steepest ascent of \n",
    "   the loss function.\n",
    "4. Train a new weak learner to minimize the gradient. The new weak learner is trained to minimize the gradient of the loss function. This means that \n",
    "   the new weak learner will focus on the samples that the model made the most mistakes on.\n",
    "5. Add the new weak learner to the model. The new weak learner is added to the model. The weights of the weak learners are determined by a process \n",
    "   called gradient descent.\n",
    "6. Repeat steps 3-5 until the desired accuracy is achieved. The process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "The mathematical intuition of gradient boosting is that the weak learners are trained to minimize the gradient of the loss function. This means that \n",
    "the weak learners are trained to focus on the samples that the model made the most mistakes on. As more and more weak learners are added to the model, \n",
    "the model becomes more accurate.\n",
    "\n",
    "Gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. It is a \n",
    "versatile algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "\n",
    "Here are some of the advantages of gradient boosting:\n",
    "\n",
    "1. Accuracy: Gradient boosting can often achieve high accuracy, especially for difficult problems.\n",
    "2. Robustness: Gradient boosting is a robust algorithm that can be used to handle noise and outliers in the data.\n",
    "3. Interpretability: Weak learners can be more interpretable than some other machine learning models. This can be helpful for understanding how the \n",
    "                     model works and for debugging the model.\n",
    "4. Scalability: Gradient boosting models can be scaled to large data sets.\n",
    "\n",
    "\n",
    "Here are some of the disadvantages of gradient boosting:\n",
    "\n",
    "1. Complexity: Gradient boosting models can be more complex than some other machine learning models. This can make them more difficult to understand \n",
    "               and to interpret.\n",
    "2. Computational complexity: Gradient boosting models can be computationally expensive to train, especially for large data sets.\n",
    "3. Overfitting: Gradient boosting models can be prone to overfitting if the hyperparameters are not tuned carefully.\n",
    "\n",
    "Overall, gradient boosting is a powerful machine learning algorithm that can be used to improve the accuracy of a variety of machine learning models. \n",
    "However, it is important to be aware of the potential disadvantages of this algorithm before using it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
